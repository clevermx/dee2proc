{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(filepath_or_buffer=meta_file_name,sep=\"\\t\")\n",
    "type(df[\"SRR_accession\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-970c36e97c84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'create_dataset' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dee2 tsv to srr h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mD:\\Tools\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1068\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m             \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Tools\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Tools\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Tools\\anaconda\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m     \"\"\"\n\u001b[0;32m    813\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "organism_name= 'ecoli'\n",
    "cache_dir=\"D://datasets/DEE2/\"\n",
    "h5_file_name = cache_dir+\"/srr_files/\"+organism_name+ \"_SRR_matrix.h5\"\n",
    "meta_file_name = cache_dir+\"/metadata/\"+organism_name+\"_metadata.tsv\"\n",
    "data_file_name = cache_dir +\"/raw_files/\"+organism_name +\"_se.tsv\"\n",
    "with h5py.File(h5_file_name, 'w') as h5_file:\n",
    "    meta_grp = h5_file.create_group('meta')\n",
    "    info_grp = h5_file.create_group('info')\n",
    "    data_grp= h5_file.create_group('data')\n",
    "    info_grp.create_dataset('version', data=\"dee2_v1\")\n",
    "    info_grp.create_dataset('creation_date', data=\"2020-02-23\")\n",
    "    meta_df=pd.read_csv(filepath_or_buffer=meta_file_name,sep=\"\\t\")\n",
    "    dt = h5py.special_dtype(vlen=np.str)\n",
    "    meta_grp.create_dataset('SRR_accession',data= np.array(meta_df[\"SRR_accession\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_geo_accession',data=np.array(meta_df[\"GSE_accession\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_instrument_model',data=np.array(meta_df[\"Model\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_last_update_date',data=np.array(meta_df[\"LoadDate\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_library_selection',data=np.array(meta_df[\"LibrarySelection\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_library_source',data=np.array(meta_df[\"LibrarySource\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_library_strategy',data=np.array(meta_df[\"LibraryStrategy\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_organism_ch1',data=np.array(meta_df[\"ScientificName\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_series_id',data=np.array(meta_df[\"Sample_name\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_status',data=np.array(meta_df[\"Consent\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_submission_date',data=np.array(meta_df[\"ReleaseDate\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_taxid_ch1',data=np.array(meta_df[\"TaxID\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_quality',data=np.array(meta_df[\"QC_summary\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_title',data=np.array(meta_df[\"BioSample\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_type',data=np.full(len(meta_df[\"BioSample\"]),\"SRA\", dtype=dt))\n",
    "    h5_file.flush()\n",
    "    n_srr=len(meta_df[\"SRR_accession\"])\n",
    "    n_genes=0\n",
    "    genes= list()\n",
    "    with open(data_file_name, 'r') as data_file:\n",
    "        srr, gene_id, gene_count= data_file.readline().split(\"\\t\")\n",
    "        first_srr = srr\n",
    "        n_genes+=1\n",
    "        genes.append(gene_id)\n",
    "        while(True):\n",
    "            srr, gene_id, gene_count= data_file.readline().split(\"\\t\")\n",
    "            if (srr!=first_srr):\n",
    "                break\n",
    "            n_genes+=1\n",
    "            genes.append(gene_id)\n",
    "        meta_grp.create_dataset('genes',data=np.array(genes, dtype='S'))\n",
    "    exp_data=data_grp.create_dataset(\"expression\", (n_srr,n_genes),dtype= 'i4')#, compression=\"gzip\", compression_opts=9)\n",
    "    h5_file.flush()\n",
    "    reader= pd.read_csv(data_file_name, sep='\\t', chunksize=n_genes,names=[\"srr\",\"gene\",\"count\"],iterator=True)\n",
    "    srr=0\n",
    "    chunk_size = 100    \n",
    "    while(srr<n_srr):\n",
    "        chunk = reader.get_chunk(chunk_size*n_genes)\n",
    "        last_ind = (srr+chunk_size) if (srr+chunk_size)<n_srr else n_srr\n",
    "        exp_data[srr: last_ind,0:n_genes]= chunk[\"count\"].values.reshape(last_ind-srr,n_genes)\n",
    "        h5_file.flush()      \n",
    "        srr+=chunk_size\n",
    "        if(srr%200==0):\n",
    "            print(srr/n_srr)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from bz2 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02847380410022779\n",
      "0.05694760820045558\n",
      "0.08542141230068337\n",
      "0.11389521640091116\n",
      "0.14236902050113895\n",
      "0.17084282460136674\n",
      "0.19931662870159453\n",
      "0.22779043280182232\n",
      "0.25626423690205014\n",
      "0.2847380410022779\n",
      "0.3132118451025057\n",
      "0.3416856492027335\n",
      "0.3701594533029613\n",
      "0.39863325740318906\n",
      "0.4271070615034169\n",
      "0.45558086560364464\n",
      "0.48405466970387245\n",
      "0.5125284738041003\n",
      "0.541002277904328\n",
      "0.5694760820045558\n",
      "0.5979498861047836\n",
      "0.6264236902050114\n",
      "0.6548974943052391\n",
      "0.683371298405467\n",
      "0.7118451025056948\n",
      "0.7403189066059226\n",
      "0.7687927107061503\n",
      "0.7972665148063781\n",
      "0.8257403189066059\n",
      "0.8542141230068337\n",
      "0.8826879271070615\n",
      "0.9111617312072893\n",
      "0.9396355353075171\n",
      "0.9681093394077449\n",
      "0.9965831435079726\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#mklever_dee2 env\n",
    "import bz2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "organism_name= 'ecoli'\n",
    "cache_dir=\"D://datasets/DEE2/\"\n",
    "h5_file_name = cache_dir+\"/srr_files/\"+organism_name+ \"_SRR_matrix.h5\"\n",
    "meta_file_name = cache_dir+\"/metadata/\"+organism_name+\"_metadata.tsv\"\n",
    "data_file_name = cache_dir +\"/raw_files/\"+organism_name +\"_se.tsv.bz2\"\n",
    "with h5py.File(h5_file_name, 'w') as h5_file:\n",
    "    meta_grp = h5_file.create_group('meta')\n",
    "    info_grp = h5_file.create_group('info')\n",
    "    data_grp= h5_file.create_group('data')\n",
    "    info_grp.create_dataset('version', data=\"dee2_v1\")\n",
    "    info_grp.create_dataset('creation_date', data=\"2020-02-23\")\n",
    "    meta_df=pd.read_csv(filepath_or_buffer=meta_file_name,sep=\"\\t\")\n",
    "    dt = h5py.special_dtype(vlen=np.str)\n",
    "    meta_grp.create_dataset('SRR_accession',data= np.array(meta_df[\"SRR_accession\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_geo_accession',data=np.array(meta_df[\"GEO_series\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_instrument_model',data=np.array(meta_df[\"Model\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_last_update_date',data=np.array(meta_df[\"LoadDate\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_library_selection',data=np.array(meta_df[\"LibrarySelection\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_library_source',data=np.array(meta_df[\"LibrarySource\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_library_strategy',data=np.array(meta_df[\"LibraryStrategy\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_organism_ch1',data=np.array(meta_df[\"ScientificName\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_series_id',data=np.array(meta_df[\"Sample_name\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_status',data=np.array(meta_df[\"Consent\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_submission_date',data=np.array(meta_df[\"ReleaseDate\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_taxid_ch1',data=np.array(meta_df[\"TaxID\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_quality',data=np.array(meta_df[\"QC_summary\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_title',data=np.array(meta_df[\"BioSample\"], dtype=dt))\n",
    "    meta_grp.create_dataset('Sample_type',data=np.full(len(meta_df[\"BioSample\"]),\"SRA\", dtype=dt))\n",
    "    h5_file.flush()\n",
    "    n_srr=len(meta_df[\"SRR_accession\"])\n",
    "    n_genes=0\n",
    "    genes= list()\n",
    "    with bz2.BZ2File(data_file_name, 'r') as data_file:\n",
    "        srr, gene_id, gene_count= data_file.readline().split(b\"\\t\")\n",
    "        first_srr = srr\n",
    "        n_genes+=1\n",
    "        genes.append(gene_id)\n",
    "        while(True):\n",
    "            srr, gene_id, gene_count= data_file.readline().split(b\"\\t\")\n",
    "            if (srr!=first_srr):\n",
    "                break\n",
    "            n_genes+=1\n",
    "            genes.append(gene_id)\n",
    "        meta_grp.create_dataset('genes',data=np.array(genes, dtype='S'))\n",
    "    exp_data=data_grp.create_dataset(\"expression\", (n_srr,n_genes),dtype= 'i4')#, compression=\"gzip\", compression_opts=9)\n",
    "    h5_file.flush()\n",
    "    reader= pd.read_csv(data_file_name, sep='\\t', chunksize=n_genes,names=[\"srr\",\"gene\",\"count\"],iterator=True)\n",
    "    srr=0\n",
    "    chunk_size = 100    \n",
    "    while(srr<n_srr):\n",
    "        chunk = reader.get_chunk(chunk_size*n_genes)\n",
    "        last_ind = (srr+chunk_size) if (srr+chunk_size)<n_srr else n_srr\n",
    "        exp_data[srr: last_ind,0:n_genes]= chunk[\"count\"].values.reshape(last_ind-srr,n_genes)\n",
    "        h5_file.flush()      \n",
    "        srr+=chunk_size\n",
    "        if(srr%200==0):\n",
    "            print(srr/n_srr)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(h5_srr_name, 'r') as h5_srr:\n",
    "    print(h5_srr[\"/meta/Sample_geo_accession\"][0].startswith(b\"GSM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8a75dfa5a525>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "test=dict()\n",
    "test.append(\"1\",1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.02847380410022779\n",
      "0.05694760820045558\n",
      "0.08542141230068337\n",
      "0.11389521640091116\n",
      "0.14236902050113895\n",
      "0.17084282460136674\n",
      "0.19931662870159453\n",
      "0.22779043280182232\n",
      "0.25626423690205014\n",
      "0.2847380410022779\n",
      "0.3132118451025057\n",
      "0.3416856492027335\n",
      "0.3701594533029613\n",
      "0.39863325740318906\n",
      "0.4271070615034169\n",
      "0.45558086560364464\n",
      "0.48405466970387245\n",
      "0.5125284738041003\n",
      "0.541002277904328\n",
      "0.5694760820045558\n",
      "0.5979498861047836\n",
      "0.6264236902050114\n",
      "0.6548974943052391\n",
      "0.683371298405467\n",
      "0.7118451025056948\n",
      "0.7403189066059226\n",
      "0.7687927107061503\n",
      "0.7972665148063781\n",
      "0.8257403189066059\n",
      "0.8542141230068337\n",
      "0.8826879271070615\n",
      "0.9111617312072893\n",
      "0.9396355353075171\n",
      "0.9681093394077449\n",
      "0.9965831435079726\n"
     ]
    }
   ],
   "source": [
    "organism_name= 'ecoli'\n",
    "cache_dir=\"D:/datasets/DEE2/\"\n",
    "h5_srr_name = cache_dir+organism_name+ \"_SRR_matrix.h5\"\n",
    "h5_gse_name = cache_dir+organism_name+ \"_matrix.h5\"\n",
    "gsm_map =dict()\n",
    "now = datetime.datetime.now()\n",
    "with h5py.File(h5_srr_name, 'r') as h5_srr:\n",
    "    with h5py.File(h5_gse_name, 'w') as h5_gse:\n",
    "        meta_grp = h5_gse.create_group('meta')\n",
    "        info_grp = h5_gse.create_group('info')\n",
    "        data_grp= h5_gse.create_group('data')\n",
    "        info_grp.create_dataset('version', data=\"dee2_gse_v1\")\n",
    "        info_grp.create_dataset('creation_date', data=now.strftime(\"%Y.%m.%d\"))\n",
    "        n_genes =len(h5_srr[\"/meta/genes\"])\n",
    "        meta_grp.create_dataset('genes',data=h5_srr[\"/meta/genes\"])\n",
    "        n_gsm=len(np.unique([x for x in h5_srr[\"/meta/Sample_geo_accession\"] if ('GSM' in x)]))\n",
    "        dt = h5py.special_dtype(vlen=np.str)\n",
    "        meta_grp.create_dataset('Sample_geo_accession',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_instrument_model',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_last_update_date',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_library_selection',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_library_source',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_library_strategy',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_organism_ch1',(n_gsm,),dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_series_id',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_status',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_submission_date',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_taxid_ch1',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_quality',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_title',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_type',data=np.full(n_gsm,\"SRA\", dtype =dt))\n",
    "        exp_data=data_grp.create_dataset(\"expression\", (n_gsm,n_genes),dtype= 'i4')#, compression=\"gzip\", compression_opts=9)\n",
    "        processed_gse=0\n",
    "        n_srr=len(h5_srr[\"/meta/SRR_accession\"])\n",
    "        for i in range(0,n_srr):\n",
    "            if(i%200==0):\n",
    "                print(i/n_srr)\n",
    "            cur_gse=processed_gse\n",
    "            if (h5_srr[\"/meta/Sample_geo_accession\"][i].startswith(\"GSM\")):\n",
    "                try:\n",
    "                    cur_gse=gsm_map[h5_srr[\"/meta/Sample_geo_accession\"][i]]                     \n",
    "                except KeyError:\n",
    "                    gsm_map[h5_srr[\"/meta/Sample_geo_accession\"][i]]=processed_gse\n",
    "                    processed_gse+=1\n",
    "                exp_data[cur_gse,0:n_genes]+=h5_srr[\"/data/expression\"][i,0:n_genes]\n",
    "                if (cur_gse==processed_gse-1):\n",
    "                    meta_grp[\"Sample_geo_accession\"][cur_gse]=h5_srr[\"/meta/Sample_geo_accession\"][i]\n",
    "                    meta_grp[\"Sample_instrument_model\"][cur_gse]=h5_srr[\"/meta/Sample_instrument_model\"][i]\n",
    "                    meta_grp[\"Sample_last_update_date\"][cur_gse]=h5_srr[\"/meta/Sample_last_update_date\"][i]\n",
    "                    meta_grp[\"Sample_library_selection\"][cur_gse]=h5_srr[\"/meta/Sample_library_selection\"][i]\n",
    "                    meta_grp[\"Sample_library_source\"][cur_gse]=h5_srr[\"/meta/Sample_library_source\"][i]\n",
    "                    meta_grp[\"Sample_library_strategy\"][cur_gse]=h5_srr[\"/meta/Sample_library_strategy\"][i]\n",
    "                    meta_grp[\"Sample_organism_ch1\"][cur_gse]=h5_srr[\"/meta/Sample_organism_ch1\"][i]\n",
    "                    meta_grp[\"Sample_status\"][cur_gse]=h5_srr[\"/meta/Sample_status\"][i]\n",
    "                    meta_grp[\"Sample_series_id\"][cur_gse]=h5_srr[\"/meta/Sample_series_id\"][i]\n",
    "                    meta_grp[\"Sample_submission_date\"][cur_gse]=h5_srr[\"/meta/Sample_submission_date\"][i]\n",
    "                    meta_grp[\"Sample_quality\"][cur_gse]=h5_srr[\"/meta/Sample_quality\"][i]\n",
    "                    meta_grp[\"Sample_title\"][cur_gse]=h5_srr[\"/meta/Sample_title\"][i]\n",
    "                    meta_grp[\"Sample_type\"][cur_gse]=h5_srr[\"/meta/Sample_type\"][i]\n",
    "        h5_gse.flush()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "srr h5 to gsm h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.06263701847792046\n",
      "0.1252740369558409\n",
      "0.18791105543376135\n",
      "0.2505480739116818\n",
      "0.31318509238960224\n",
      "0.3758221108675227\n",
      "0.4384591293454432\n",
      "0.5010961478233636\n",
      "0.563733166301284\n",
      "0.6263701847792045\n",
      "0.689007203257125\n",
      "0.7516442217350454\n",
      "0.8142812402129659\n",
      "0.8769182586908864\n",
      "0.9395552771688067\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "def unique_fil(srr_group,gse_group,cur_gse,dset,ind):\n",
    "    cur_vals=srr_group[dset][ind]\n",
    "    if (len(np.unique(cur_vals))!=1):\n",
    "        gse_group[dset][cur_gse]=\",\".join(cur_vals)\n",
    "    else:\n",
    "        gse_group[dset][cur_gse]=cur_vals[0]\n",
    "\n",
    "organism_name= 'ecoli'\n",
    "cache_dir=\"D:/datasets/DEE2/\"\n",
    "h5_srr_name = cache_dir+organism_name+ \"_SRR_matrix.h5\"\n",
    "h5_gsm_name = cache_dir+organism_name+ \"_matrix.h5\"\n",
    "gsm_map =defaultdict(list)\n",
    "now = datetime.datetime.now()\n",
    "gsm_list=[]\n",
    "with h5py.File(h5_srr_name, 'r') as h5_srr:\n",
    "    with h5py.File(h5_gsm_name, 'w') as h5_gse:\n",
    "        meta_grp = h5_gsm.create_group('meta')\n",
    "        info_grp = h5_gsm.create_group('info')\n",
    "        data_grp= h5_gsm.create_group('data')\n",
    "        info_grp.create_dataset('version', data=\"dee2_gse_v1\")\n",
    "        info_grp.create_dataset('creation_date', data=now.strftime(\"%Y.%m.%d\"))\n",
    "        n_genes =len(h5_srr[\"/meta/genes\"])\n",
    "        meta_grp.create_dataset('genes',data=h5_srr[\"/meta/genes\"])\n",
    "        for i_gsm in range(h5_srr[\"/meta/Sample_geo_accession\"].shape[0]):\n",
    "            row = h5_srr[\"/meta/Sample_geo_accession\"][i_gsm]\n",
    "            if ('GSM' in row):\n",
    "                gsm_map[row].append(i_gsm)                      \n",
    "        n_gsm=len(gsm_map)\n",
    "        dt = h5py.special_dtype(vlen=np.str)\n",
    "        meta_grp.create_dataset('Sample_geo_accession',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('SRR_accession',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_instrument_model',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_last_update_date',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_library_selection',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_library_source',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_library_strategy',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_organism_ch1',(n_gsm,),dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_series_id',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_status',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_submission_date',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_taxid_ch1',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_quality',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_title',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_type',data=np.full(n_gsm,\"SRA\", dtype =dt))\n",
    "        exp_data=data_grp.create_dataset(\"expression\", (n_gsm,n_genes),dtype= 'i4')#, compression=\"gzip\", compression_opts=9)\n",
    "        gsm_list = list(gsm_map.keys())\n",
    "        for i_gsm in  range(len(gsm_list)):\n",
    "            cur_gsm=gsm_list[i_gsm]\n",
    "            meta_grp[\"Sample_geo_accession\"][i_gsm]=cur_gsm\n",
    "            for key in h5_srr[\"meta\"]:\n",
    "                if (key == \"genes\"):\n",
    "                    continue\n",
    "                unique_fil(h5_srr[\"meta\"],meta_grp,i_gsm,key,gsm_map[cur_gsm])\n",
    "            exp_data[i_gsm,0:n_genes]=np.sum(h5_srr[\"/data/expression\"][gsm_map[cur_gsm],0:n_genes],0)\n",
    "            if(i_gsm%200==0):\n",
    "                print(i_gsm/len(gsm_list))\n",
    "        h5_gse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39297\n",
      "180869\n",
      "180869\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sparse matrices for srr and tx transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'D:/datasets/DEE2/TxInfo/hsapiens_TxInfo.tsv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mD:\\Tools\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Tools\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Tools\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Tools\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Tools\\anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'D:/datasets/DEE2/TxInfo/hsapiens_TxInfo.tsv' does not exist"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import bz2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "cache_dir=\"D:/datasets/DEE2/\"\n",
    "organism_name= 'hsapiens'#'hsapiens' 'ecoli'\n",
    "geo_acc_name = 'Sample_name' # may be \"GSE_accession\" or \"Sample_name\"\n",
    "tx_info_file = cache_dir+\"TxInfo/\"+ organism_name+\"_TxInfo.tsv\"\n",
    "metadata_file = cache_dir+\"metadata/\"+ organism_name+\"_metadata.tsv\"\n",
    "\n",
    "tx_info=pd.read_csv(filepath_or_buffer=tx_info_file,sep=\"\\t\")\n",
    "tx_to_gene= dict(zip(tx_info[\"TxID\"],tx_info[\"GeneID\"]))\n",
    "tx_ind= {tx:ind for ind,tx in enumerate(tx_to_gene.keys())}\n",
    "gene_ind= {gene:ind for ind,gene in enumerate(pd.unique(tx_info[\"GeneID\"]))}\n",
    "tx_to_gene_matrix= sps.lil_matrix((len(tx_ind),len(gene_ind)))\n",
    "for tx_key in tx_to_gene:\n",
    "    tx_to_gene_matrix[tx_ind[tx_key],gene_ind[tx_to_gene[tx_key]]]=1\n",
    "print(len(gene_ind))\n",
    "print(len(tx_ind))\n",
    "print(len(tx_to_gene))\n",
    "#meta_df=pd.read_csv(filepath_or_buffer=metadata_file,sep=\"\\t\")   \n",
    "iter_meta = pd.read_csv(filepath_or_buffer=metadata_file,sep=\"\\t\", iterator=True, chunksize=1000)\n",
    "\n",
    "#meta_df = pd.concat([chunk[chunk[geo_acc_name].str.startswith('GSM')] for chunk in iter_meta])\n",
    "meta_df = pd.concat([chunk for chunk in iter_meta])\n",
    "true_srr=len(meta_df)\n",
    "print(true_srr)\n",
    "meta_df=meta_df[meta_df[geo_acc_name].str.startswith('GSM')]\n",
    "n_srr= len(meta_df)\n",
    "print(n_srr)\n",
    "srr_to_gsm= dict(zip(meta_df[\"SRR_accession\"],meta_df[geo_acc_name]))\n",
    "srr_ind= {srr:ind for ind,srr in enumerate(srr_to_gsm.keys())}\n",
    "gsm_ind= {gsm:ind for ind,gsm in enumerate(pd.unique(meta_df[geo_acc_name]))}\n",
    "gsm_from_srr_matrix= sps.lil_matrix((len(gsm_ind),len(srr_ind)))\n",
    "for srr_key in srr_to_gsm:\n",
    "    gsm_from_srr_matrix[gsm_ind[srr_to_gsm[srr_key]],srr_ind[srr_key]]=1\n",
    "print(gsm_from_srr_matrix[0,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dee2,transform via sparse matrices and write to h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14236902050113895\n",
      "0.2847380410022779\n",
      "0.4271070615034169\n",
      "0.5694760820045558\n",
      "0.7118451025056948\n",
      "0.8542141230068337\n",
      "0.9965831435079726\n",
      "1\n",
      "Wall time: 40.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import datetime\n",
    "from scipy.sparse import isspmatrix_csr,csr_matrix,csc_matrix\n",
    "data_file_name= cache_dir+\"raw_files/\"+organism_name+\"_ke.tsv.bz2\"\n",
    "h5_gsm_name = cache_dir+organism_name+ \"_kalisto_matrix.h5\"\n",
    "now = datetime.datetime.now()\n",
    "with h5py.File(h5_gsm_name, 'w') as h5_gse:\n",
    "    meta_grp = h5_gse.create_group('meta')\n",
    "    info_grp = h5_gse.create_group('info')\n",
    "    data_grp= h5_gse.create_group('data')\n",
    "    info_grp.create_dataset('version', data=\"dee2_gse_v1\")\n",
    "    info_grp.create_dataset('creation_date', data=now.strftime(\"%Y.%m.%d\"))\n",
    "    n_genes =len(gene_ind)\n",
    "    dt = h5py.special_dtype(vlen=np.str)\n",
    "    meta_grp.create_dataset('genes',data=gene_ind.keys(),dtype=dt)\n",
    "    n_gsm=len(gsm_ind)        \n",
    "    meta_grp.create_dataset('Sample_geo_accession',data=gsm_ind.keys(), dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_instrument_model',(n_gsm,), dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_last_update_date',(n_gsm,), dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_library_selection',(n_gsm,), dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_library_source',(n_gsm,), dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_library_strategy',(n_gsm,), dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_organism_ch1',(n_gsm,),dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_series_id',(n_gsm,), dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_status',(n_gsm,), dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_submission_date',(n_gsm,), dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_taxid_ch1',(n_gsm,), dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_quality',(n_gsm,), dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_title',(n_gsm,), dtype =dt)\n",
    "    meta_grp.create_dataset('Sample_type',data=np.full(n_gsm,\"SRA\", dtype =dt))\n",
    "    exp_data=data_grp.create_dataset(\"expression\", (n_gsm,n_genes),dtype= 'i4')#, compression=\"gzip\", compression_opts=9)\n",
    "    processed_gse=0\n",
    "    srr_per_time=1000\n",
    "    row_length= len(tx_ind)\n",
    "    iter_data = pd.read_csv(filepath_or_buffer=data_file_name,sep=\"\\t\", iterator=True, chunksize=srr_per_time*row_length,names=[\"srr\",\"tx\",\"ke\"])\n",
    "    proc_srr= 0\n",
    "    gsm_from_srr_matrix=csr_matrix(gsm_from_srr_matrix)    \n",
    "    tx_to_gene_matrix=csr_matrix(tx_to_gene_matrix)\n",
    "    for chunk in iter_data:\n",
    "        global_ids =[]\n",
    "        local_ids=[]\n",
    "        local_srr_ind= {srr:ind for ind,srr in enumerate(pd.unique(chunk['srr']))}\n",
    "        srr_count=len(local_srr_ind)\n",
    "        proc_srr=proc_srr+srr_count\n",
    "        for cur_srr in local_srr_ind:\n",
    "            try:\n",
    "                srr_id= srr_ind[cur_srr]\n",
    "                global_ids.append(srr_id)\n",
    "                local_ids.append(local_srr_ind[cur_srr])\n",
    "            except(KeyError):\n",
    "                continue\n",
    "        if local_ids:           \n",
    "            raw_matrix= chunk[\"ke\"].values.astype(int).reshape(srr_count,row_length)\n",
    "            A=gsm_from_srr_matrix[:,global_ids]\n",
    "            gsm_mask=[np.sum(x)>0 for x in A]            \n",
    "            A = A.tocsr()\n",
    "            A = A.dot(raw_matrix[local_ids,])\n",
    "            A=csr_matrix(A)\n",
    "            A = A.dot(tx_to_gene_matrix)\n",
    "            exp_data[gsm_mask== True,0:n_genes]= exp_data[gsm_mask== True,0:n_genes]+A[gsm_mask== True,0:n_genes]\n",
    "        if proc_srr%500 ==0:\n",
    "            print(proc_srr/true_srr)\n",
    "    h5_gse.flush()\n",
    "print(1)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "15\n",
      "24\n",
      "3\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "a=np.array([1,2,3,4,5,6,7,8,9]).reshape(3,3)\n",
    "a\n",
    "for i in a:\n",
    "    print(np.sum(i))\n",
    "b=np.matrix('1 2; 3 4')\n",
    "for i in b:\n",
    "    print(np.sum(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dee2 kalisto to h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009882162459588544\n",
      "0.01976432491917709\n",
      "0.029646487378765633\n",
      "0.03952864983835418\n",
      "0.04941081229794272\n",
      "0.059292974757531265\n",
      "0.0691751372171198\n",
      "0.07905729967670835\n",
      "0.08893946213629689\n",
      "0.09882162459588544\n",
      "0.10870378705547398\n",
      "0.11858594951506253\n",
      "0.12846811197465108\n",
      "0.1383502744342396\n",
      "0.14823243689382815\n",
      "0.1581145993534167\n",
      "0.16799676181300524\n",
      "0.17787892427259377\n",
      "0.18776108673218234\n",
      "0.19764324919177087\n",
      "0.2075254116513594\n",
      "0.21740757411094797\n",
      "0.2272897365705365\n",
      "0.23717189903012506\n",
      "0.2470540614897136\n",
      "0.25693622394930216\n",
      "0.26681838640889066\n",
      "0.2767005488684792\n",
      "0.2865827113280678\n",
      "0.2964648737876563\n",
      "0.30634703624724485\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Tools\\anaconda\\lib\\site-packages\\h5py\\_hl\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[1;31m# === Check for zero-sized datasets =====\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m             \u001b[1;31m# These are the only access methods NumPy allows for such objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mEllipsis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import bz2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import datetime\n",
    "import math\n",
    "cache_dir=\"D:/datasets/DEE2/\"\n",
    "organism_name= 'ecoli'\n",
    "tx_info_file = cache_dir+\"TxInfo/\"+ organism_name+\"_TxInfo.tsv\"\n",
    "metadata_file = cache_dir+\"metadata/\"+ organism_name+\"_metadata.tsv\"\n",
    "data_file_name= cache_dir+\"raw_files/\"+organism_name+\"_ke.tsv.bz2\"\n",
    "h5_gsm_name = cache_dir+organism_name+ \"_kalisto_matrix.h5\"\n",
    "#check txs\n",
    "#tx_dict=dict()\n",
    "#with open(tx_info_file, 'r') as info_file:\n",
    "#    for line in info_file:\n",
    "#        tx_id, gene_id, *tail= line.split(\"\\t\")\n",
    "#        try:\n",
    "#            if (tx_dict[tx_id] != gene_id):\n",
    "#                print(\"Error. tx in few genes\")\n",
    "#                break\n",
    "#        except(KeyError):\n",
    "#            tx_dict[tx_id] = gene_id\n",
    "#\n",
    "#            \n",
    "#print(len(tx_dict))\n",
    "\n",
    "\n",
    "tx_info=pd.read_csv(filepath_or_buffer=tx_info_file,sep=\"\\t\")\n",
    "tx_to_gene= dict(zip(tx_info[\"TxID\"],tx_info[\"GeneID\"]))\n",
    "genes= pd.unique(tx_info[\"GeneID\"])\n",
    "genes=dict(zip(genes,range(0,len(genes))))\n",
    "tx_g_shape = (len(tx_to_gene), len(genes))\n",
    "tx_to_gene_matrix= sps.coo_matrix(tx_g_shape)\n",
    "for (tx_key in tx_to_gene):\n",
    "    tx_to_gene_matrix[]\n",
    "\n",
    "meta_df=pd.read_csv(filepath_or_buffer=metadata_file,sep=\"\\t\")\n",
    "n_srr= len(meta_df)\n",
    "srr_to_gsm= dict(zip(meta_df[\"SRR_accession\"],meta_df[\"GSE_accession\"]))\n",
    "gsms= pd.unique([x for x in meta_df[\"GSE_accession\"] if ('GSM' in x)])\n",
    "gsms= dict(zip(gsms,range(0,len(gsms))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(h5_gsm_name, 'w') as h5_gse:\n",
    "        meta_grp = h5_gse.create_group('meta')\n",
    "        info_grp = h5_gse.create_group('info')\n",
    "        data_grp= h5_gse.create_group('data')\n",
    "        info_grp.create_dataset('version', data=\"dee2_gse_v1\")\n",
    "        info_grp.create_dataset('creation_date', data=now.strftime(\"%Y.%m.%d\"))\n",
    "        n_genes =len(genes)\n",
    "        dt = h5py.special_dtype(vlen=np.str)\n",
    "        meta_grp.create_dataset('genes',data=genes.keys(),dtype=dt)\n",
    "        n_gsm=len(gsms)        \n",
    "        meta_grp.create_dataset('Sample_geo_accession',data=gsms.keys(), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_instrument_model',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_last_update_date',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_library_selection',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_library_source',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_library_strategy',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_organism_ch1',(n_gsm,),dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_series_id',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_status',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_submission_date',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_taxid_ch1',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_quality',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_title',(n_gsm,), dtype =dt)\n",
    "        meta_grp.create_dataset('Sample_type',data=np.full(n_gsm,\"SRA\", dtype =dt))\n",
    "        exp_data=data_grp.create_dataset(\"expression\", (n_gsm,n_genes),dtype= 'i4')#, compression=\"gzip\", compression_opts=9)\n",
    "        processed_gse=0\n",
    "        with bz2.open(data_file_name, 'rt') as data_file:\n",
    "            proc_srr=0\n",
    "            n_srr= 30357728\n",
    "            for line in data_file:\n",
    "                srr,tx,ke=line.split(\"\\t\")\n",
    "                proc_srr+=1\n",
    "                if(proc_srr%300000==0):\n",
    "                    print(proc_srr/n_srr)\n",
    "                cur_gsm=srr_to_gsm[srr]\n",
    "                try:\n",
    "                    gsm_ind=gsms[cur_gsm]\n",
    "                    cur_gene=tx_to_gene[tx]\n",
    "                    gene_ind=genes[cur_gene]\n",
    "                    exp_data[gsm_ind,gene_ind]=exp_data[gsm_ind,gene_ind]+math.floor(float(ke))    \n",
    "                except(KeyError):\n",
    "                    continue\n",
    "                            \n",
    "                \n",
    "        h5_gse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3193\n",
      "<class 'dict'>\n",
      "<class 'numpy.ndarray'>\n",
      "7024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.1111"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsms= pd.unique([x for x in meta_df[\"GSE_accession\"] if ('GSM' in x)])\n",
    "print(len(gsms))\n",
    "print(type(genes))\n",
    "print(type(gsms))\n",
    "gsms= dict(zip(gsms,range(0,len(gsms))))\n",
    "#print(gsms.keys())\n",
    "print(len(meta_df))\n",
    "genes=dict(zip(genes,range(0,len(genes))))\n",
    "\n",
    "meta_df[\"SampleName\"]\n",
    "float('3.1111\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qwe', 'ry']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d= defaultdict(list)\n",
    "d[\"qwe\"].append(1)\n",
    "d[\"qwe\"].append(10)\n",
    "d[\"ry\"].append(2)\n",
    "print(list(d.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   digit  qwe\n",
      "0    111  aaa\n",
      "1    222  bbb\n",
      "2    333  ccc\n",
      "3    444  ddd\n",
      "4    555  eee\n",
      "   digit  qwe\n",
      "5    666  fff\n",
      "6    777  ggg\n",
      "7    888  hhh\n",
      "8    999  iii\n",
      "<pandas.io.parsers.TextFileReader object at 0x000001D4E811E390>\n",
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reader = pd.read_csv(\"D:/datasets/test.txt\", sep='\\t',names=[\"digit\",\"qwe\"], iterator=True)\n",
    "\n",
    "print(reader.get_chunk(5))\n",
    "print(reader.get_chunk(10))\n",
    "print(reader)\n",
    "print([1,2,3,4,5,5,6,78,8][1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   167726\n",
      "   35238\n",
      "Sample_channel_count\n",
      "Sample_characteristics_ch1\n",
      "Sample_contact_address\n",
      "Sample_contact_city\n",
      "Sample_contact_country\n",
      "Sample_contact_department\n",
      "Sample_contact_email\n",
      "Sample_contact_institute\n",
      "Sample_contact_laboratory\n",
      "Sample_contact_name\n",
      "Sample_contact_phone\n",
      "Sample_contact_zip-postal_code\n",
      "Sample_data_processing\n",
      "Sample_data_row_count\n",
      "Sample_description\n",
      "Sample_extract_protocol_ch1\n",
      "Sample_geo_accession\n",
      "Sample_instrument_model\n",
      "Sample_last_update_date\n",
      "Sample_library_selection\n",
      "Sample_library_source\n",
      "Sample_library_strategy\n",
      "Sample_molecule_ch1\n",
      "Sample_organism_ch1\n",
      "Sample_platform_id\n",
      "Sample_relation\n",
      "Sample_series_id\n",
      "Sample_source_name_ch1\n",
      "Sample_status\n",
      "Sample_submission_date\n",
      "Sample_supplementary_file_1\n",
      "Sample_supplementary_file_2\n",
      "Sample_taxid_ch1\n",
      "Sample_title\n",
      "Sample_type\n",
      "gene_accession\n",
      "gene_chromosome\n",
      "gene_entrezid\n",
      "gene_hgnc\n",
      "gene_name\n",
      "gene_refseqid\n",
      "gene_synonym\n",
      "genes\n",
      "reads_aligned\n",
      "total_reads\n",
      "35238\n"
     ]
    }
   ],
   "source": [
    "arch_file = \"D:/datasets/ARCHS4_v7/human_matrix.h5\"\n",
    "with h5py.File(arch_file, 'a') as arch_7:\n",
    "    print(\"   \" + str(len(arch_7[\"data\"][\"expression\"])))\n",
    "    print( \"   \" + str(len(arch_7[\"data\"][\"expression\"][1])))\n",
    "    for key in arch_7[\"meta\"].keys():\n",
    "        print(key)\n",
    "    print(len(arch_7[\"meta\"][\"genes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   238522\n",
      "   35238\n",
      "Sample_channel_count\n",
      "Sample_characteristics_ch1\n",
      "Sample_contact_address\n",
      "Sample_contact_city\n",
      "Sample_contact_country\n",
      "Sample_contact_department\n",
      "Sample_contact_email\n",
      "Sample_contact_institute\n",
      "Sample_contact_laboratory\n",
      "Sample_contact_name\n",
      "Sample_contact_phone\n",
      "Sample_contact_zip-postal_code\n",
      "Sample_data_processing\n",
      "Sample_data_row_count\n",
      "Sample_description\n",
      "Sample_extract_protocol_ch1\n",
      "Sample_geo_accession\n",
      "Sample_instrument_model\n",
      "Sample_last_update_date\n",
      "Sample_library_selection\n",
      "Sample_library_source\n",
      "Sample_library_strategy\n",
      "Sample_molecule_ch1\n",
      "Sample_organism_ch1\n",
      "Sample_platform_id\n",
      "Sample_relation\n",
      "Sample_series_id\n",
      "Sample_source_name_ch1\n",
      "Sample_status\n",
      "Sample_submission_date\n",
      "Sample_supplementary_file_1\n",
      "Sample_supplementary_file_2\n",
      "Sample_taxid_ch1\n",
      "Sample_title\n",
      "Sample_type\n",
      "gene_accession\n",
      "gene_chromosome\n",
      "gene_entrezid\n",
      "gene_hgnc\n",
      "gene_name\n",
      "gene_refseqid\n",
      "gene_synonym\n",
      "genes\n",
      "reads_aligned\n",
      "reads_total\n",
      "35238\n"
     ]
    }
   ],
   "source": [
    "arch_file_8 = \"D:/datasets/phantasus_cache/archs4/human_matrix_8.h5\"\n",
    "with h5py.File(arch_file_8, 'a') as arch_8:\n",
    "    print(\"   \" + str(len(arch_8[\"data\"][\"expression\"])))\n",
    "    print( \"   \" + str(len(arch_8[\"data\"][\"expression\"][1])))\n",
    "    for key in arch_8[\"meta\"].keys():\n",
    "        print(key)\n",
    "    print(len(arch_8[\"meta\"][\"genes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'KeysView' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-34693910aa2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0march_7\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0march_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0march_8\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0march_file_8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0march_7\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"meta\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0march_8\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"meta\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m \u001b[0march_7\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"meta\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'KeysView' object does not support indexing"
     ]
    }
   ],
   "source": [
    "arch_7= h5py.File(arch_file, 'a')\n",
    "arch_8= h5py.File(arch_file_8, 'a')\n",
    "\n",
    "arch_7[\"meta\"].keys()[arch_8[\"meta\"].keys()!= arch_7[\"meta\"].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
